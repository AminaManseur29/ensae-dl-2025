{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST with an MLP, from scratch\n",
    "\n",
    "# - Step 1: build an MLP from scratch to solve MNIST. Question set: https://fleuret.org/dlc/materials/dlc-practical-3.pdf\n",
    "# - Step 2: debug your network with backprop ninja and a reference implementation using torch's .backward()\n",
    "# - Step 3: build the same MLP but will full pytorch code (nn.Linear, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target, test_input, test_target = load_data(one_hot_labels = True, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x225c4411210>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_input[4].view((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy (preds, targets):\n",
    "    \"\"\" Computes the accuracy between predictions and targets. Data is expected to be one-hot encoded. \"\"\"\n",
    "    _, idx1 = torch.max(preds, dim=1)\n",
    "    _, idx2 = torch.max(targets, dim=1)\n",
    "    d = idx1 == idx2\n",
    "    return d.int().float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unit test\n",
    "# this cell should return 0.75\n",
    "preds = torch.zeros((4,7))\n",
    "preds[0,1] = 1\n",
    "preds[1,4] = 1\n",
    "preds[2,2] = 1\n",
    "preds[3,6] = 1\n",
    "targets = torch.zeros((4,7))\n",
    "targets[0,1] = 1\n",
    "targets[1,4] = 1\n",
    "targets[2,2] = 1\n",
    "targets[3,2] = 1\n",
    "compute_accuracy(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(x):\n",
    "    return torch.tanh(x)\n",
    "\n",
    "def dsigma(x):\n",
    "    return 1 - torch.pow(sigma(x), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss (v,t):\n",
    "    return torch.sum(torch.pow(v-t, 2))\n",
    "\n",
    "def dloss(v,t):\n",
    "    return -2 * (t - v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5787, -0.9364,  3.6656,  0.7376, -1.5734,  2.3179],\n",
       "        [-2.0180,  1.5956,  0.2792, -2.0212,  2.1317, -0.4389],\n",
       "        [-3.9487,  3.3172,  2.7199, -0.5366, -1.0881, -0.2018]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "v = torch.randn((3, 6), dtype=torch.float32)\n",
    "t = torch.randn((3, 6), dtype=torch.float32)\n",
    "l=loss(v,t)\n",
    "dloss(v,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply targets by 0.9 to be in the range of tanh\n",
    "train_target *= 0.9\n",
    "test_target *= 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Backprop ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "# DO NOT MODIFY IT\n",
    "#\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "w1 = torch.randn((784, 50))\n",
    "b1 = torch.randn((50))\n",
    "w2 = torch.randn((50, 10))\n",
    "b2 = torch.randn((10))\n",
    "parameters = [w1, b1, w2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 10]), tensor(43.2825, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = train_input[:5]\n",
    "y1 = train_target[:5]\n",
    "z1 = x1@w1 + b1\n",
    "h1 = sigma(z1)\n",
    "z2 = h1@w2 + b2\n",
    "h2 = sigma(z2)\n",
    "l = loss(h2, y1)\n",
    "h2.shape, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=43.28252410888672\n"
     ]
    }
   ],
   "source": [
    "# Force pytorch to retain grade for intermediate nodes and reset grad for parameters\n",
    "# DO NOT MODIFY THIS CODE\n",
    "#\n",
    "others = [h2,z2,h1,z1]\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in others:\n",
    "    t.retain_grad()\n",
    "l.backward()\n",
    "print(f'loss={l}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "z2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "w2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "z1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "w1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# here we compare our gradient to the reference gradient computed by pytorch\n",
    "dl = 1.0\n",
    "dh2 = dloss(h2, y1)*dl\n",
    "cmp('h2',dh2,h2)\n",
    "dz2 = dsigma(z2)*dh2\n",
    "cmp('z2',dz2, z2)\n",
    "dw2 = h1.T @ dz2\n",
    "cmp('w2',dw2, w2)\n",
    "db2 = dz2.sum(axis=0, keepdim=True)\n",
    "cmp('b2',db2, b2)\n",
    "dh1 = dz2 @ w2.T\n",
    "cmp('h1',dh1, h1)\n",
    "dz1 = dsigma(z1)*dh1\n",
    "cmp('z1', dz1, z1)\n",
    "dw1 = x1.T @ dz1\n",
    "cmp('w1', dw1, w1)\n",
    "db1 = dz1.sum(axis=0, keepdim=True)\n",
    "cmp('b1', db1, b1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "with torch.no_grad():\n",
    "    w1 += -lr * dw1\n",
    "    b1 += -lr * db1.squeeze()\n",
    "    w2 += -lr * dw2\n",
    "    b2 += -lr * db2.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.28252410888672"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = loss(h2, y1)\n",
    "l.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now that we've checked our gradients are correct, we can implement the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(w1, b1, w2, b2, x):\n",
    "    z1 = x @ w1 + b1\n",
    "    h1 = sigma(z1)\n",
    "    z2 = h1 @ w2 + b2\n",
    "    h2 = sigma(z2)\n",
    "    return z1, h1, z2, h2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(w1, b1, w2, b2, x1, y1, h2, z2, h1, z1):\n",
    "    dl = 1.0\n",
    "    dh2 = dloss(h2, y1)*dl\n",
    "    dz2 = dsigma(z2)*dh2\n",
    "    dw2 = h1.T @ dz2\n",
    "    db2 = dz2.sum(axis=0, keepdim=True)\n",
    "    dh1 = dz2 @ w2.T\n",
    "    dz1 = dsigma(z1)*dh1\n",
    "    dw1 = x1.T @ dz1\n",
    "    db1 = dz1.sum(axis=0, keepdim=True)\n",
    "    return dw1, db1, dw2, db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(w1, b1, w2, b2, dw1, db1, dw2, db2, lr):\n",
    "    with torch.no_grad():\n",
    "        w1 += -lr * dw1\n",
    "        b1 += -lr * db1.squeeze()\n",
    "        w2 += -lr * dw2\n",
    "        b2 += -lr * db2.squeeze()\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    \"\"\" init a network \"\"\"\n",
    "    torch.manual_seed(1337)\n",
    "    w1 = torch.randn((784, 50))\n",
    "    b1 = torch.randn((50))\n",
    "    w2 = torch.randn((50, 10))\n",
    "    b2 = torch.randn((10))\n",
    "    return w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, b1, w2, b2 = init()\n",
    "parameters = [w1, b1, w2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main training loop\n",
    "torch.set_printoptions(linewidth=200)\n",
    "def train(w1, b1, w2, b2):\n",
    "    lossi = []\n",
    "    for step in range(10000):\n",
    "        xb = train_input\n",
    "        yb = train_target\n",
    "        num_samples = xb.shape[0]\n",
    "        # forward\n",
    "        z1, h1, z2, h2 = forward(w1, b1, w2, b2, xb)\n",
    "        lsi = loss(h2, yb)\n",
    "        # backward\n",
    "        dw1, db1, dw2, db2 = backward(w1, b1, w2, b2, xb, yb, h2, z2, h1, z1)\n",
    "        # update\n",
    "        lr = 0.1 / num_samples if step < 5000 else 0.01 / num_samples\n",
    "        w1, b1, w2, b2 = update(w1, b1, w2, b2, dw1, db1, dw2, db2, lr)\n",
    "        if step % 100 == 0: print(f'step = {step}, loss = {lsi}')\n",
    "        lossi.append(lsi.item())\n",
    "    # compute accuracy\n",
    "    _, _, _, preds = forward(w1, b1, w2, b2, train_input)\n",
    "    train_accuracy = compute_accuracy(preds, train_target)\n",
    "    _, _, _, preds = forward(w1, b1, w2, b2, test_input)\n",
    "    test_accuracy = compute_accuracy(preds, test_target)\n",
    "    print(f'{train_accuracy=}')\n",
    "    print(f'{test_accuracy=}')\n",
    "    return lossi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0, loss = 9739.25390625\n",
      "step = 100, loss = 8378.8603515625\n",
      "step = 200, loss = 7969.4765625\n",
      "step = 300, loss = 7732.75830078125\n",
      "step = 400, loss = 7516.67236328125\n",
      "step = 500, loss = 7350.58740234375\n",
      "step = 600, loss = 7158.3310546875\n",
      "step = 700, loss = 7018.76171875\n",
      "step = 800, loss = 6871.544921875\n",
      "step = 900, loss = 6743.244140625\n",
      "step = 1000, loss = 6618.8232421875\n",
      "step = 1100, loss = 6478.99267578125\n",
      "step = 1200, loss = 6340.7646484375\n",
      "step = 1300, loss = 6193.2763671875\n",
      "step = 1400, loss = 5956.68505859375\n",
      "step = 1500, loss = 5526.3896484375\n",
      "step = 1600, loss = 5326.87841796875\n",
      "step = 1700, loss = 5044.02392578125\n",
      "step = 1800, loss = 4533.06494140625\n",
      "step = 1900, loss = 3999.675048828125\n",
      "step = 2000, loss = 3202.3828125\n",
      "step = 2100, loss = 2254.830078125\n",
      "step = 2200, loss = 2160.728515625\n",
      "step = 2300, loss = 2102.804931640625\n",
      "step = 2400, loss = 2041.9456787109375\n",
      "step = 2500, loss = 1975.2200927734375\n",
      "step = 2600, loss = 1879.4619140625\n",
      "step = 2700, loss = 1698.953369140625\n",
      "step = 2800, loss = 1390.988525390625\n",
      "step = 2900, loss = 1338.6392822265625\n",
      "step = 3000, loss = 1202.578125\n",
      "step = 3100, loss = 940.9425659179688\n",
      "step = 3200, loss = 920.2731323242188\n",
      "step = 3300, loss = 900.806884765625\n",
      "step = 3400, loss = 882.7625122070312\n",
      "step = 3500, loss = 866.7138061523438\n",
      "step = 3600, loss = 850.0997314453125\n",
      "step = 3700, loss = 834.885986328125\n",
      "step = 3800, loss = 821.5317993164062\n",
      "step = 3900, loss = 809.3276977539062\n",
      "step = 4000, loss = 795.3079833984375\n",
      "step = 4100, loss = 776.7548828125\n",
      "step = 4200, loss = 750.5711059570312\n",
      "step = 4300, loss = 727.1563720703125\n",
      "step = 4400, loss = 699.3463745117188\n",
      "step = 4500, loss = 664.4820556640625\n",
      "step = 4600, loss = 601.1219482421875\n",
      "step = 4700, loss = 455.57940673828125\n",
      "step = 4800, loss = 423.38116455078125\n",
      "step = 4900, loss = 414.6617736816406\n",
      "step = 5000, loss = 409.947021484375\n",
      "step = 5100, loss = 408.1168518066406\n",
      "step = 5200, loss = 407.861083984375\n",
      "step = 5300, loss = 407.60955810546875\n",
      "step = 5400, loss = 407.3622131347656\n",
      "step = 5500, loss = 407.11865234375\n",
      "step = 5600, loss = 406.8788146972656\n",
      "step = 5700, loss = 406.6426086425781\n",
      "step = 5800, loss = 406.4097900390625\n",
      "step = 5900, loss = 406.1802062988281\n",
      "step = 6000, loss = 405.9537658691406\n",
      "step = 6100, loss = 405.7300109863281\n",
      "step = 6200, loss = 405.5091247558594\n",
      "step = 6300, loss = 405.2907409667969\n",
      "step = 6400, loss = 405.07470703125\n",
      "step = 6500, loss = 404.8608093261719\n",
      "step = 6600, loss = 404.6488037109375\n",
      "step = 6700, loss = 404.43853759765625\n",
      "step = 6800, loss = 404.2298278808594\n",
      "step = 6900, loss = 404.0224609375\n",
      "step = 7000, loss = 403.81640625\n",
      "step = 7100, loss = 403.6114196777344\n",
      "step = 7200, loss = 403.4073791503906\n",
      "step = 7300, loss = 403.2040710449219\n",
      "step = 7400, loss = 403.0014343261719\n",
      "step = 7500, loss = 402.7994689941406\n",
      "step = 7600, loss = 402.5979309082031\n",
      "step = 7700, loss = 402.3968505859375\n",
      "step = 7800, loss = 402.196044921875\n",
      "step = 7900, loss = 401.99554443359375\n",
      "step = 8000, loss = 401.795166015625\n",
      "step = 8100, loss = 401.5950012207031\n",
      "step = 8200, loss = 401.3949279785156\n",
      "step = 8300, loss = 401.1949768066406\n",
      "step = 8400, loss = 400.9951477050781\n",
      "step = 8500, loss = 400.7955017089844\n",
      "step = 8600, loss = 400.5959167480469\n",
      "step = 8700, loss = 400.3965759277344\n",
      "step = 8800, loss = 400.19744873046875\n",
      "step = 8900, loss = 399.9984436035156\n",
      "step = 9000, loss = 399.7995300292969\n",
      "step = 9100, loss = 399.60076904296875\n",
      "step = 9200, loss = 399.4020690917969\n",
      "step = 9300, loss = 399.2034606933594\n",
      "step = 9400, loss = 399.00482177734375\n",
      "step = 9500, loss = 398.80615234375\n",
      "step = 9600, loss = 398.6072998046875\n",
      "step = 9700, loss = 398.408203125\n",
      "step = 9800, loss = 398.20880126953125\n",
      "step = 9900, loss = 398.00909423828125\n",
      "train_accuracy=0.8240000009536743\n",
      "test_accuracy=0.5899999737739563\n"
     ]
    }
   ],
   "source": [
    "lossi = train(w1, b1, w2, b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x225c4f22610>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGdCAYAAADwjmIIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA8/klEQVR4nO3deXxU9b3/8fdMlkkgmSQs2SBsoiyCslWMoF6vKVGjLdUuaGqpolYLVsSrSBW0dYFitVetSrWtcOvu71argCgXFKqGAJEdDChbBJMAITMJkHW+vz9CDgwEDJDJmZm8no/HPDzLd2Y+5wsy78c53+85DmOMEQAAQJhx2l0AAABAIBByAABAWCLkAACAsETIAQAAYYmQAwAAwhIhBwAAhCVCDgAACEuEHAAAEJYi7S7ATj6fT7t371Z8fLwcDofd5QAAgGYwxqiiokLp6elyOk98vqZNh5zdu3crIyPD7jIAAMBpKCoqUteuXU+4v02HnPj4eEkNneR2u22uBgAANIfX61VGRob1O34ibTrkNF6icrvdhBwAAELMdw01OeWBx0uXLtU111yj9PR0ORwOvfvuu377jTGaNm2a0tLSFBsbq6ysLG3ZssWvTVlZmXJzc+V2u5WYmKhx48apsrLSr83atWt18cUXKyYmRhkZGZo5c+Zxtbz99tvq27evYmJiNHDgQM2fP/9UDwcAAISpUw45Bw4c0Pnnn6/nnnuuyf0zZ87UM888o1mzZik/P1/t27dXdna2qqqqrDa5ubnasGGDFi5cqLlz52rp0qW67bbbrP1er1ejRo1S9+7dVVBQoCeeeEIPP/ywXnzxRavN559/ruuvv17jxo3TqlWrNHr0aI0ePVrr168/1UMCAADhyJwBSeadd96x1n0+n0lNTTVPPPGEta28vNy4XC7z+uuvG2OM2bhxo5FkVqxYYbX54IMPjMPhMLt27TLGGPP888+bpKQkU11dbbWZPHmy6dOnj7X+05/+1OTk5PjVM3z4cPOrX/2q2fV7PB4jyXg8nma/BwAA2Ku5v98tep+cbdu2qbi4WFlZWda2hIQEDR8+XHl5eZKkvLw8JSYmatiwYVabrKwsOZ1O5efnW20uueQSRUdHW22ys7NVWFio/fv3W22O/p7GNo3f05Tq6mp5vV6/FwAACE8tGnKKi4slSSkpKX7bU1JSrH3FxcVKTk722x8ZGakOHTr4tWnqM47+jhO1adzflOnTpyshIcF6MX0cAIDw1abueDxlyhR5PB7rVVRUZHdJAAAgQFo05KSmpkqSSkpK/LaXlJRY+1JTU1VaWuq3v66uTmVlZX5tmvqMo7/jRG0a9zfF5XJZ08WZNg4AQHhr0ZDTs2dPpaamatGiRdY2r9er/Px8ZWZmSpIyMzNVXl6ugoICq83ixYvl8/k0fPhwq83SpUtVW1trtVm4cKH69OmjpKQkq83R39PYpvF7AABA23bKIaeyslKrV6/W6tWrJTUMNl69erV27twph8OhiRMn6tFHH9V7772ndevW6Re/+IXS09M1evRoSVK/fv10xRVX6NZbb9Xy5cv12WefacKECRozZozS09MlSTfccIOio6M1btw4bdiwQW+++aaefvppTZo0yarjrrvu0oIFC/Tkk0/qyy+/1MMPP6yVK1dqwoQJZ94rAAAg9J3qtK2PP/7YSDruNXbsWGNMwzTyqVOnmpSUFONyuczll19uCgsL/T5j37595vrrrzdxcXHG7Xabm266yVRUVPi1WbNmjRk5cqRxuVymS5cuZsaMGcfV8tZbb5lzzjnHREdHm3PPPdfMmzfvlI6FKeQAAISe5v5+O4wxxsaMZSuv16uEhAR5PB7G5wAAECKa+/vdpmZXAQCAtqNNP6AzUJ76qFCeQ7Uaf1lvJbtj7C4HAIA2iTM5AfD6iiLNyduhvZU1dpcCAECbRcgJgOiIhm6tqffZXAkAAG0XIScAoiMburWWkAMAgG0IOQFgncmpI+QAAGAXQk4AREU6JHG5CgAAOxFyAoAzOQAA2I+QEwBREYzJAQDAboScAGgceMyZHAAA7EPICQAuVwEAYD9CTgAwhRwAAPsRcgKgMeRUcyYHAADbEHICICYyQpJUVVtvcyUAALRdhJwAiItpeO5pRXWdzZUAANB2EXICoL2rIeQcIOQAAGAbQk4AxB8OOZVVhBwAAOxCyAmAxstVldWMyQEAwC6EnABovFxVWV1rcyUAALRdhJwAsC5XMSYHAADbEHIC4MjAYy5XAQBgF0JOAMQdDjkVDDwGAMA2hJwAiGMKOQAAtiPkBED84dlVh2rreX4VAAA2IeQEQGPIkbhkBQCAXQg5ARAZ4bQuWZUdqLG5GgAA2iZCToCkuF2SpFJvlc2VAADQNhFyAiQ1IUaSVEzIAQDAFoScAElxN4Scbz2EHAAA7EDICZC0w2dySjiTAwCALQg5AZJ6+ExOMWdyAACwBSEnQBovV3EmBwAAexByAiQtIVYSY3IAALALISdAUhIappDvraxWHXc9BgCg1RFyAqRTe5cinQ75jLSnstrucgAAaHMIOQHidDqYRg4AgI0IOQHUeNfjEkIOAACtjpATQNz1GAAA+xByAqimrmHA8T/ydthcCQAAbQ8hJ4DaRTc8iXzr3gM2VwIAQNtDyAmgrkmxdpcAAECbRcgJoPO6JljLVbX1NlYCAEDbQ8gJoFH9U63lHfsO2lgJAABtDyEngJxOh7okNlyyWrG9zOZqAABoWwg5Abar/JAk6cF319tcCQAAbQshJ8B+NLiLJFlndAAAQOsg5ATYVQPTJB05owMAAFoHISfAOrSPspYPVNfZWAkAAG0LISfABmUkWctz1+62sRIAANoWQk6ARTgd1vKu/VyyAgCgtRByWkG/NLckaTv3ygEAoNUQclpB39R4SdJ7a7hcBQBAayHktAKfMdayOWoZAAAEDiGnFTyQ089a3lxSaWMlAAC0HYScVpAcH2MtZ//3UhsrAQCg7SDk2KC23md3CQAAhD1Cjg2WFO6xuwQAAMIeIaeVLL33Mmv51n+stLESAADaBkJOK+nWsZ21zAQrAAACj5DTivofvimgxFRyAAACjZDTit741YXW8tR/rbexEgAAwl+Lh5z6+npNnTpVPXv2VGxsrM466yw98sgjfmcujDGaNm2a0tLSFBsbq6ysLG3ZssXvc8rKypSbmyu3263ExESNGzdOlZX+95hZu3atLr74YsXExCgjI0MzZ85s6cNpUe6YI08kf2XZThsrAQAg/LV4yPnDH/6gF154QX/+85+1adMm/eEPf9DMmTP17LPPWm1mzpypZ555RrNmzVJ+fr7at2+v7OxsVVVVWW1yc3O1YcMGLVy4UHPnztXSpUt12223Wfu9Xq9GjRql7t27q6CgQE888YQefvhhvfjiiy19SC2qXXSEtXzNs5/aWAkAAOHNYVp4cMjVV1+tlJQU/e1vf7O2XXfddYqNjdUrr7wiY4zS09N1zz336L/+678kSR6PRykpKZo9e7bGjBmjTZs2qX///lqxYoWGDRsmSVqwYIGuuuoqffPNN0pPT9cLL7ygBx54QMXFxYqOjpYk3X///Xr33Xf15ZdfNqtWr9erhIQEeTweud3u735DC6ip8+mcBz+w1rfPyGmV7wUAIFw09/e7xc/kXHTRRVq0aJE2b94sSVqzZo0+/fRTXXnllZKkbdu2qbi4WFlZWdZ7EhISNHz4cOXl5UmS8vLylJiYaAUcScrKypLT6VR+fr7V5pJLLrECjiRlZ2ersLBQ+/fvb7K26upqeb1ev1dri4707/KvSitavQYAANqCFg85999/v8aMGaO+ffsqKipKgwcP1sSJE5WbmytJKi4uliSlpKT4vS8lJcXaV1xcrOTkZL/9kZGR6tChg1+bpj7j6O841vTp05WQkGC9MjIyzvBoT8/dWedYy1lP8ZgHAAACocVDzltvvaVXX31Vr732mr744gvNmTNHf/zjHzVnzpyW/qpTNmXKFHk8HutVVFRkSx3jLzvLb/3jwlJb6gAAIJxFtvQH3nvvvdbZHEkaOHCgduzYoenTp2vs2LFKTU2VJJWUlCgtLc16X0lJiQYNGiRJSk1NVWmp/w9/XV2dysrKrPenpqaqpKTEr03jemObY7lcLrlcrjM/yDMUGeGfLW96eQVjcwAAaGEtfibn4MGDcjr9PzYiIkI+X8NDKXv27KnU1FQtWrTI2u/1epWfn6/MzExJUmZmpsrLy1VQUGC1Wbx4sXw+n4YPH261Wbp0qWpra602CxcuVJ8+fZSUlNTSh9Xivpj6fb/18oM1NlUCAEB4avGQc8011+ixxx7TvHnztH37dr3zzjt66qmn9KMf/UiS5HA4NHHiRD366KN67733tG7dOv3iF79Qenq6Ro8eLUnq16+frrjiCt16661avny5PvvsM02YMEFjxoxRenq6JOmGG25QdHS0xo0bpw0bNujNN9/U008/rUmTJrX0IQVEh/bRfuuDfr/QpkoAAAhPLT6FvKKiQlOnTtU777yj0tJSpaen6/rrr9e0adOsmVDGGD300EN68cUXVV5erpEjR+r555/XOeccGZBbVlamCRMm6P3335fT6dR1112nZ555RnFxcVabtWvXavz48VqxYoU6deqkO++8U5MnT252rXZMIT9aUdlBXTzzY2v9vQkjdF7XxFavAwCAUNLc3+8WDzmhxO6QI0mDfv+Ryg8eueTG2BwAAE7Otvvk4NQsve8yv/WvSitP0BIAAJwKQo7N3DFRunlET2s966kl2n+AQcgAAJwpQk4QmHJVX7/1wY8wCBkAgDNFyAkCURFOXXGu/719qmrrbaoGAIDwQMgJEs/nDvFb7zt1gU2VAAAQHgg5QcLpdOjrx6/y21Zdx9kcAABOFyEniEQ4HRrRu6O13ufBBfIcNb0cAAA0HyEnyPzlxmF+6+f//iObKgEAILQRcoJMnCtSPxna1W9baUWVTdUAABC6CDlBaMZ15/mtX/DYItXV+2yqBgCA0ETICUIRTodeGTfcb9sH64ttqgYAgNBEyAlSI8/u5Ld+5+urVO9rs48ZAwDglBFygthXj13pt37Wb+fbVAkAAKGHkBPEIiOcxwWdz7/aa1M1AACEFkJOkIuMcOqFo+6GfMNf8+XjshUAAN+JkBMCrhyY5rfei8tWAAB8J0JOiPhi6vf91j/jshUAACdFyAkRHdpHa+6dI6313L/m62BNnY0VAQAQ3Ag5IWRAlwS/9f7TPpS3imdbAQDQFEJOiNl6zJPKz3uYZ1sBANAUQk6IcTodeu0W/7shz/l8uz3FAAAQxAg5Ieii3p0049qB1vpD722wsRoAAIITISdEjbmgm996j/vn2VQJAADBiZATwtY9PMpvfdzsFTZVAgBA8CHkhLD4mCjNufkCa33Rl6Uq9VbZWBEAAMGDkBPiLj2ns0b1T7HWL3h8EY99AABAhJyw8Jcbh/qt9/rtfBlD0AEAtG2EnDDgcDiOu39Ozyk83woA0LYRcsKE0+nQ/026xG/b79/faFM1AADYj5ATRnonx+uDuy621v/+2Tb1m7rAxooAALAPISfM9Etz6+3bM631Q7X1+nBDsY0VAQBgD0JOGPpejw765UU9rPVf/aNA+w/U2FcQAAA2IOSEqYd/cK7uzjrHWh/8yEIdqqm3sSIAAFoXISeM3ZV1ttwxkdZ6v2kLVFfvs7EiAABaDyEnzK19ONtvvfcDH+j/NpbYVA0AAK2HkNMGHHsPnVv+Z6V2lx+yqRoAAFoHIacNcDod2jbdP+hcNGOxdhF0AABhjJDTRjgcDm2fkeO3bcSMxTznCgAQtgg5bcyxQafXb+ernqADAAhDhJw26Nigc9Zv5+ur0gqbqgEAIDAIOW3U18cMRs56aqkqqmptqgYAgJZHyGmjIpzHP7l84MMf2VQNAAAtj5DThjmdDn0x9ft+23rcP0/GMEYHABD6CDltXIf20ced0ek5Zb5N1QAA0HIIOZDT6dCm31/ht+2Hf/7UpmoAAGgZhBxIkmKjI1TwYJa1vuYbjy547P9U6q2ysSoAAE4fIQeWjnEuLX/gcmu9tKJaFzy+yMaKAAA4fYQc+EmOj9H/3nGR37Ye98+zqRoAAE4fIQfHGdo9SW/fnum37ed/zbepGgAATg8hB036Xo8OevIn51vrn361V899/JWNFQEAcGoIOTih64Z21dDuSdb6Ex8W6pG5G22sCACA5iPk4KSOHZ/zt0+3aV9ltU3VAADQfIQcfKdjH+g59NH/I+gAAIIeIQfN0lTQ4fEPAIBgRshBsx0bdHj8AwAgmBFycEqODTqT/99amyoBAODkCDk4ZV89dqW1/ObKIq3f5bGxGgAAmkbIwSmLjHBq7p0jrfWrn/2U8TkAgKBDyMFpGdAlQb+6pJe1/ti8TTZWAwDA8Qg5OG1Truqn3slxkqS/frpN33oO2VwRAABHEHJwRub/5mJr+dInPrGvEAAAjhGQkLNr1y79/Oc/V8eOHRUbG6uBAwdq5cqV1n5jjKZNm6a0tDTFxsYqKytLW7Zs8fuMsrIy5ebmyu12KzExUePGjVNlZaVfm7Vr1+riiy9WTEyMMjIyNHPmzEAcDk4iOtKpOTdfIEmqqfNpyeY9NlcEAECDFg85+/fv14gRIxQVFaUPPvhAGzdu1JNPPqmkpCPPQJo5c6aeeeYZzZo1S/n5+Wrfvr2ys7NVVVVltcnNzdWGDRu0cOFCzZ07V0uXLtVtt91m7fd6vRo1apS6d++ugoICPfHEE3r44Yf14osvtvQh4Ttcek5na3ns35fbWAkAAEcxLWzy5Mlm5MiRJ9zv8/lMamqqeeKJJ6xt5eXlxuVymddff90YY8zGjRuNJLNixQqrzQcffGAcDofZtWuXMcaY559/3iQlJZnq6mq/7+7Tp0+za/V4PEaS8Xg8zX4PmvbS0q9N98lzTffJc03+1n12lwMACGPN/f1u8TM57733noYNG6af/OQnSk5O1uDBg/XSSy9Z+7dt26bi4mJlZWVZ2xISEjR8+HDl5eVJkvLy8pSYmKhhw4ZZbbKysuR0OpWfn2+1ueSSSxQdHW21yc7OVmFhofbv399kbdXV1fJ6vX4vtIxbLj4y0+qmlzmbAwCwX4uHnK1bt+qFF17Q2WefrQ8//FB33HGHfvOb32jOnDmSpOLiYklSSkqK3/tSUlKsfcXFxUpOTvbbHxkZqQ4dOvi1aeozjv6OY02fPl0JCQnWKyMj4wyPFke77fCU8gM19TpUU29zNQCAtq7FQ47P59OQIUP0+OOPa/Dgwbrtttt06623atasWS39VadsypQp8ng81quoqMjuksLKfdl9rOWX/r3VxkoAAAhAyElLS1P//v39tvXr1087d+6UJKWmpkqSSkpK/NqUlJRY+1JTU1VaWuq3v66uTmVlZX5tmvqMo7/jWC6XS2632++FlhMZ4bTumzPn8+32FgMAaPNaPOSMGDFChYWFfts2b96s7t27S5J69uyp1NRULVq0yNrv9XqVn5+vzMxMSVJmZqbKy8tVUFBgtVm8eLF8Pp+GDx9utVm6dKlqa2utNgsXLlSfPn38ZnKhdf3+h+dKkvYdqOGSFQDAVi0ecu6++24tW7ZMjz/+uL766iu99tprevHFFzV+/HhJksPh0MSJE/Xoo4/qvffe07p16/SLX/xC6enpGj16tKSGMz9XXHGFbr31Vi1fvlyfffaZJkyYoDFjxig9PV2SdMMNNyg6Olrjxo3Thg0b9Oabb+rpp5/WpEmTWvqQcAoye3W0lj/a2PTYKAAAWkUgpna9//77ZsCAAcblcpm+ffuaF1980W+/z+czU6dONSkpKcblcpnLL7/cFBYW+rXZt2+fuf76601cXJxxu93mpptuMhUVFX5t1qxZY0aOHGlcLpfp0qWLmTFjxinVyRTywMh9aZnpPnmuefi99XaXAgAIQ839/XYY03YfH+31epWQkCCPx8P4nBb0vwXf6J6316hfmlsf3HXxd78BAIBT0Nzfb55dhRZ3Ue+GS1abvvWqsrrO5moAAG0VIQctLi0hVmkJMZKkgh1N35gRAIBAI+QgIIZ2b5jhlvf1PpsrAQC0VYQcBMTgbg0h5/Ov99pcCQCgrSLkICAGpDcMBFv7jUd19T6bqwEAtEWEHARE4+UqSVpVVG5fIQCANouQg4CIjHCqU5xLkvTSUp5jBQBofYQcBMz3+zc8Ff6jjSXf0RIAgJZHyEHA3HJxT2v5nVXf2FgJAKAtIuQgYM7qHGct3/3mGhsrAQC0RYQcBNQLuUOs5ec+/srGSgAAbQ0hBwF15cA0a/mJDwtVWlFlYzUAgLaEkIOAy5vyn9byBY8tsrESAEBbQshBwKUlxKp/2pGnxP558RYbqwEAtBWEHLSK+XddbC3/8aPNqqqtt7EaAEBbQMhBq/n3fZdZy9c8+6mNlQAA2gJCDlpNRod2yjk8EHlLaaXW7/LYXBEAIJwRctCqnr1+sLV8NWdzAAABRMhBq3I6HXp6zCBrfcnmPfYVAwAIa4QctLofDupiLY/9+3IbKwEAhDNCDmxx9J2Q877eZ2MlAIBwRciBLY6+E/L1Ly2zsRIAQLgi5MA2M64daC2XeHncAwCgZRFyYJsxF3Szlu/9f2ttrAQAEI4IObDVzy9sCDpLN++Rz2dsrgYAEE4IObDV5Cv6WssfbSy2sRIAQLgh5MBW8TFRSnG7JEkv/XubzdUAAMIJIQe2m5h1jiSpYMd+1db7bK4GABAuCDmw3Y+HdrWWl23lnjkAgJZByIHtoiKc+sH56ZKkD9YzLgcA0DIIOQgKOec13Bxw8aZSGcMsKwDAmSPkIChccnZnRUc4Veyt0pbSSrvLAQCEAUIOgkJsdISG9+ogSfp0y16bqwEAhANCDoLGyN6dJEn/3rLH5koAAOGAkIOgMeJwyFm5Y7/qufsxAOAMEXIQNPqmxis+JlIVVXXauNtrdzkAgBBHyEHQiIxwamj3JEnSFzv321wNACDUEXIQVAZnNISclTsIOQCAM0PIQVAZ1qMh5KwuIuQAAM4MIQdBZUCXBElSUdkhlR2osbkaAEAoI+QgqCTERqlXp/aSOJsDADgzhBwEnfMzEiVJ675hhhUA4PQRchB0Gi9ZrdvlsbkSAEAoI+Qg6Aw8HHLWflPOwzoBAKeNkIOgM7BLgiKcDpVWVOtbT5Xd5QAAQhQhB0EnNjpCfVPjJUmri8rtLQYAELIIOQhKg7slSpIKuCkgAOA0EXIQlL7Xo4MkadnWfTZXAgAIVYQcBKXMszpKkjZ+61X5QW4KCAA4dYQcBKXk+Bj1To6TMdKyrWV2lwMACEGEHAStiw6fzcn7eq/NlQAAQhEhB0Hrwl4NISd/G2dyAACnjpCDoHVBz4bBx18WV2g/D+sEAJwiQg6CVqc4l3onx0mSlm/nbA4A4NQQchDUhh8+m5PP4GMAwCki5CCoDbfG5XC/HADAqSHkIKhdePhMzsZvvfIcqrW5GgBAKCHkIKglu2PUs1N7GSOtZFwOAOAUEHIQ9KxxOUwlBwCcAkIOgt7wXo2DjxmXAwBoPkIOgt7wng2Dj9fv9qqyus7magAAoSLgIWfGjBlyOByaOHGita2qqkrjx49Xx44dFRcXp+uuu04lJSV+79u5c6dycnLUrl07JScn695771Vdnf8P3CeffKIhQ4bI5XKpd+/emj17dqAPBzZIT4xVRodY1fuMCnbst7scAECICGjIWbFihf7yl7/ovPPO89t+99136/3339fbb7+tJUuWaPfu3br22mut/fX19crJyVFNTY0+//xzzZkzR7Nnz9a0adOsNtu2bVNOTo4uu+wyrV69WhMnTtQtt9yiDz/8MJCHBJuc1yVRkrS5uMLeQgAAISNgIaeyslK5ubl66aWXlJSUZG33eDz629/+pqeeekr/+Z//qaFDh+rll1/W559/rmXLlkmSPvroI23cuFGvvPKKBg0apCuvvFKPPPKInnvuOdXUNNzef9asWerZs6eefPJJ9evXTxMmTNCPf/xj/elPfwrUIcFGZx2+8/HXeyptrgQAECoCFnLGjx+vnJwcZWVl+W0vKChQbW2t3/a+ffuqW7duysvLkyTl5eVp4MCBSklJsdpkZ2fL6/Vqw4YNVptjPzs7O9v6jKZUV1fL6/X6vRAaGh/v8FUpIQcA0DyRgfjQN954Q1988YVWrFhx3L7i4mJFR0crMTHRb3tKSoqKi4utNkcHnMb9jftO1sbr9erQoUOKjY097runT5+u3/3ud6d9XLDPWZ3bS5K+2lMpY4wcDofNFQEAgl2Ln8kpKirSXXfdpVdffVUxMTEt/fFnZMqUKfJ4PNarqKjI7pLQTGd1jpPDIZUfrFUZTyQHADRDi4ecgoIClZaWasiQIYqMjFRkZKSWLFmiZ555RpGRkUpJSVFNTY3Ky8v93ldSUqLU1FRJUmpq6nGzrRrXv6uN2+1u8iyOJLlcLrndbr8XQkNMVIS6JjX8uXLJCgDQHC0eci6//HKtW7dOq1evtl7Dhg1Tbm6utRwVFaVFixZZ7yksLNTOnTuVmZkpScrMzNS6detUWlpqtVm4cKHcbrf69+9vtTn6MxrbNH4Gwk/vzo2Djw/YXAkAIBS0+Jic+Ph4DRgwwG9b+/bt1bFjR2v7uHHjNGnSJHXo0EFut1t33nmnMjMzdeGFF0qSRo0apf79++vGG2/UzJkzVVxcrAcffFDjx4+Xy+WSJN1+++3685//rPvuu08333yzFi9erLfeekvz5s1r6UNCkOjesb2kPdpZdtDuUgAAISAgA4+/y5/+9Cc5nU5dd911qq6uVnZ2tp5//nlrf0REhObOnas77rhDmZmZat++vcaOHavf//73VpuePXtq3rx5uvvuu/X000+ra9eu+utf/6rs7Gw7DgmtIKNDO0lS0X5CDgDguzmMMcbuIuzi9XqVkJAgj8fD+JwQ8NGGYt32jwKd1zVB700YaXc5AACbNPf3m2dXIWR063j4TA6XqwAAzUDIQcjISGoIOfsP1qqiqtbmagAAwY6Qg5DR3hWpDu2jJUlFZYdsrgYAEOwIOQgpjYOPmWEFAPguhByElIzDNwT8hhlWAIDvQMhBSOnGmRwAQDMRchBSrHvlEHIAAN+BkIOQwpkcAEBzEXIQUhqnkX+z/5B8vjZ7H0sAQDMQchBS0hJjFOF0qLrOpz2V1XaXAwAIYoQchJSoCKfSEmIkMS4HAHByhByEHMblAACag5CDkNM4Loe7HgMAToaQg5DT+KBOzuQAAE6GkIOQ06Nje0nS5pIKmysBAAQzQg5CznldEyRJXxZ7VVVbb3M1AIBgRchByOmaFKuO7aNVW2+0YbfX7nIAAEGKkIOQ43A4NLhboiTpix377S0GABC0CDkISd/r0UGSlL+tzOZKAADBipCDkDTscMhZtXO/jOHxDgCA4xFyEJIGdHErOtKpfQdqtG3vAbvLAQAEIUIOQpIrMkLndWmYZVXAuBwAQBMIOQhZQ7snSZK+2FlubyEAgKBEyEHIGtytIeSs2smZHADA8Qg5CFmNZ3IKSyrkOVhrczUAgGBDyEHI6hzvUrcO7WSMtH63x+5yAABBhpCDkHZuuluStIGQAwA4BiEHIa1fWkPIKSyutLkSAECwIeQgpHXv2E6SVLT/oM2VAACCDSEHIa1rUkPI+aaMkAMA8EfIQUjL6BArSfrWW6WaOp/N1QAAggkhByGtc5xLMVFOGSPtLj9kdzkAgCBCyEFIczgc1iUrxuUAAI5GyEHIy0hquGRVVMaZHADAEYQchLyMDpzJAQAcj5CDkJfReLmKGVYAgKMQchDyuhy+XPWtp8rmSgAAwYSQg5CX4nZJkkorCDkAgCMIOQh5yfExkqQSb7WMMTZXAwAIFoQchLzO8Q1ncmrqfPIcqrW5GgBAsCDkIOTFREUosV2UJKm0otrmagAAwYKQg7CQYl2yYlwOAKABIQdhIblx8LGXMzkAgAaEHIQFa/AxM6wAAIcRchAWUjiTAwA4BiEHYSE1oeFMTjE3BAQAHEbIQVhIcR8OOQw8BgAcRshBWEh1cyYHAOCPkIOwkHb4ctWeymrV+7jrMQCAkIMw0THOpQinQ/U+o72VDD4GABByECYinA4lH368A08jBwBIhByEEWZYAQCORshB2Dgy+PiQzZUAAIIBIQdho2tSrCRpR9lBmysBAAQDQg7CRu/kOEnSV6WVNlcCAAgGhByEjcaQ8zUhBwAgQg7CyDkp8YpwOrTbU6UiLlkBQJsXaXcBQEuJj4nS0O5JWr6tTBfP/Nhv3/YZOTZVBQCwC2dyEFZ+PLRrk9t73D9PPe6f18rVAADsRMhBWPnJ0K763Q/OPeH+xrBTXVffilUBAOzgMMa02Qf9eL1eJSQkyOPxyO12210OAsAYo55T5p9w/9bHr5LT6WjFigAAZ6q5v98tfiZn+vTp+t73vqf4+HglJydr9OjRKiws9GtTVVWl8ePHq2PHjoqLi9N1112nkpISvzY7d+5UTk6O2rVrp+TkZN17772qq6vza/PJJ59oyJAhcrlc6t27t2bPnt3Sh4MQ53A4tH1GjrbPyFGnuOjj9vf67Xz1uH+eqmo5swMA4abFQ86SJUs0fvx4LVu2TAsXLlRtba1GjRqlAwcOWG3uvvtuvf/++3r77be1ZMkS7d69W9dee621v76+Xjk5OaqpqdHnn3+uOXPmaPbs2Zo2bZrVZtu2bcrJydFll12m1atXa+LEibrlllv04YcftvQhIUysfPD72j4jR7dfetZx+/pOXaAe989TZXVdE+8EAISigF+u2rNnj5KTk7VkyRJdcskl8ng86ty5s1577TX9+Mc/liR9+eWX6tevn/Ly8nThhRfqgw8+0NVXX63du3crJSVFkjRr1ixNnjxZe/bsUXR0tCZPnqx58+Zp/fr11neNGTNG5eXlWrBgQbNq43JV2zbhtS80d+23Te7jMhYABC/bLlcdy+PxSJI6dOggSSooKFBtba2ysrKsNn379lW3bt2Ul5cnScrLy9PAgQOtgCNJ2dnZ8nq92rBhg9Xm6M9obNP4GU2prq6W1+v1e6Ht+vMNQ7R9Ro5m/XzIcfsaL2MBAEJXQEOOz+fTxIkTNWLECA0YMECSVFxcrOjoaCUmJvq1TUlJUXFxsdXm6IDTuL9x38naeL1eHTrU9AMap0+froSEBOuVkZFxxseI0HfFgDRtn5Gjp8cMOm5fj/vnyedrs2PzASCkBTTkjB8/XuvXr9cbb7wRyK9ptilTpsjj8VivoqIiu0tCEPnhoC7aPiNHV5+X5re912/n63/ytttTFADgtAUs5EyYMEFz587Vxx9/rK5dj9ygLTU1VTU1NSovL/drX1JSotTUVKvNsbOtGte/q43b7VZsbGyTNblcLrndbr8XcKw/3zBE26Zf5bdt2r82cPkKAEJMi4ccY4wmTJigd955R4sXL1bPnj399g8dOlRRUVFatGiRta2wsFA7d+5UZmamJCkzM1Pr1q1TaWmp1WbhwoVyu93q37+/1eboz2hs0/gZwJlonHp+LIIOAISOFp9d9etf/1qvvfaa/vWvf6lPnz7W9oSEBOsMyx133KH58+dr9uzZcrvduvPOOyVJn3/+uaSGKeSDBg1Senq6Zs6cqeLiYt1444265ZZb9Pjjj0tqmEI+YMAAjR8/XjfffLMWL16s3/zmN5o3b56ys7ObVSuzq9Acy7eV6ad/8R/QzrOwAMA+zf39bvGQ43A0Pe325Zdf1i9/+UtJDTcDvOeee/T666+rurpa2dnZev75561LUZK0Y8cO3XHHHfrkk0/Uvn17jR07VjNmzFBk5JFnin7yySe6++67tXHjRnXt2lVTp061vqM5CDloroM1deo/7cg9mMaN7KmpV/e3sSIAaLtsCzmhhJCDU3Gguk7nPnQk6BQ8mKWOcS4bKwKAtilo7pMDhIv2rki9O36EtT700f+zsRoAwHch5ACnYFBGorokHpm9t+lbbigJAMGKkAOcok8nX2Yt3zJnpY2VAABOhpADnCKHw6F7vn+OJGlX+SEdrOGhngAQjAg5wGn49WW9reVXlu2wsRIAwIkQcoDTEOF06D/6dJYkFezYb3M1AICmEHKA03TziIa7eX/21T7V8xBPAAg6hBzgNI3o3UlxrkhVVtdp7TfldpcDADgGIQc4TRFOhy7s1UGSlLd1n83VAACORcgBzsCFvTpKklZuZ1wOAAQbQg5wBob1aDiTk7+VcTkAEGwIOcAZGJDuVpwrUgdq6rV08x67ywEAHIWQA5yByAinhnRPkiTdNHuFzdUAAI5GyAHO0NUD06zlHvfP0y9fXq7d5YdsrAgAIBFygDP2k2Fd/dY/Kdyji2Ys1kP/Wi9vVa1NVQEACDnAGXI4HNr6+FXHbZ+Tt0Pff2qJ/r2FsToAYAeHMabNTgnxer1KSEiQx+OR2+22uxyEkX+t3qUZH3ypbz1VkqR7vn+OJvxnbzkcDpsrA4DQ19zfb87kAAHww0FdtHDSpbp2SBdJ0pMLN+v+/10nH9PMAaDVEHKAAIlzReqpnw7SI6MHyOmQ3lxZpN+9v0Ft+OQpALQqQg4QYDde2F1/+tkgSQ3jdP73i132FgQAbQQhB2gFPxzURZO+f44k6eH3NmhvZbXNFQFA+CPkAK1k/GW9dW66W5XVdfqfvB12lwMAYY+QA7SSCKdDd/zHWZKkf+Rt16GaepsrAoDwRsgBWtEV56Yqo0Os9h+s1f8rKLK7HAAIa4QcoBVFRjh1y8hekqSXP9vOTCsACCBCDtDKfjy0q2KjIrR17wGtKiq3uxwACFuEHKCVtXdF6ooBqZKkd5hODgABQ8gBbPCjwQ13Qn5/7W7V1PlsrgYAwhMhB7DBiN6dlOJ2qfxgrf73i2/sLgcAwhIhB7BBhNOh2y5pmE7+5EeF3BwQAAIg0u4CgLbqxgu769VlO7R17wGN+tNSXXNems5OideB6jq5Y6N0dnKc+qTGKz4myu5SASAkOUwbnsPa3Ee1A4GyuaRCt/+jQFv3Hjhhm1R3jLp3bKeendqrV+f26p0cp24d2qtrUqxioiJasVoACA7N/f0m5BByYLPaep+WFO7Rks179K2nSq4op0q9VdpcUinPodqTvrdzvEsZSbHqmtRO3Tu2U0ZSO3XtEKvuHdsr1R2jCKejlY4CAFoPIacZCDkIdvsqq7V93wFt33tQ2/cd0Nd7KrV1zwEVlR3Uge94LER0hFNdkmKt8JOeGKu0hBjrv53jXZwJAhCSmvv7zZgcIIh1jHOpY5xLQ7t38NtujFHZgRp9s/+QdpUf0s6yg9pZdlBFZQf1zf5D+mb/QdXU+7Rt7wFtO8mlsI7to9U53qVkd4yS413qHO9SyuH1FLdLaQmxSo53KTKCOQoAQg8hBwhBDofDCkDnZyQet7/eZ7T7qPDzzf6D+ra8Srs9h7S7vErFnirV1Pu070CN9h2o0ZfFFSf8LqdDSkuIVbcO7dSzc3v1ahwb1DleXZNi5eSSGIAgxeUqLlehDTLGqPxgrb71VKm0okqlFdXac/hV4m1YL/ZUqcRbpTrfif+JiHNFqn+6W4O7JWpY9w4a3C1RneJcrXgkANoixuQ0AyEHOLl6n9GeimrtKj+o7XsPWpe/vt5Tqa17DzR5t+ZUd4z6pcWrb5pb/dPcOq9rgrp1aCeHgzM+AFoGIacZCDnA6aur9+nrPQe05ptyFWzfr1VF+7WltFJN/YvSsX20Lj67ky7t01kXn92Zsz0AzgghpxkIOUDLqqiq1eaSCm36tkKbvvVq/W6vNu32qqbe/4zPwC4JuuScTrr0nGQN7paoKAY2AzgFhJxmIOQAgVdT59Oqnfu1ZHPDvYA27Pb67Y93RWrk2Z10w/BuGtm7E5e1AHwnQk4zEHKA1ldaUaV/b96rpVv2aOnmPdp/8MgND4d2T9LjPxqoPqnxNlYIINgRcpqBkAPYq95ntH6XR++s2qU3VxTpUG29Ip0O/f6HA3TD8G52lwcgSDX395sL4QBsE+F06PyMRD38g3O1+L8uVVa/FNX5jH77zjq9mr/D7vIAhDhCDoCgkJYQq5d+MVR3/MdZkqSp767X51/ttbkqAKGMkAMgaDgcDt2X3UfXDu4in5HufH2VdpcfsrssACGKkAMgqDgcDj1+7UD1T3Nr34Ea3fHqF6quO/nDSAGgKYQcAEEnJipCs34+VAmxUVpTVK6731ytYk+V3WUBCDHMrmJ2FRC0Pi4s1bjZK+QzDQ8K7dU57vADQuOU4nYpzhWp+JhItXdFHrfcPjqSh4cCYYop5M1AyAGCX/7WfXryo81avr3slN/bLjpCEY03Fzwq7zQuNt540HGyfUd93pF2J3tf4/qRnceW0NQND602fp/p/z3+tRxT3ykew7Gf7V+L//v8ajrJ+x3HNGr6e/2/o6nvcTRxME3Wcmz/nGTf0U56fCdo49/uNP/8j913yn9m/m382jXr793R33fivjv2+Jr79+dEf7/vGXWO4mOi1JKa+/sd2aLfCgAtbHivjnrr9kwVe6pUWFKhrXsqtW3vAe07UKPKqjodqK5T5dGvqjrryekHaxjLA9ht/GW9FR9jz3cTcgCEhNSEGKUmxOjSczqftJ0xRtV1PlVW1+lgdb18xsgcs1+Stc3/XLbx2+b/vsZt/m2+a9+J2jT1PUefWD++vpO977vf79cLJ3h/U8dwsr7Tyd53/Ncd/34df3xNv+/4YzhZ3x3fpon3nenxneR7mmqjYz7b/+/PSfYd06aJj2z67+QJ2hzb7tjPPtHfraa+p7nH1y464rjaWwshB0BYcTgciomKUExUhBRndzUA7MTsKgAAEJYIOQAAICwRcgAAQFgi5AAAgLBEyAEAAGGJkAMAAMISIQcAAIQlQg4AAAhLIR9ynnvuOfXo0UMxMTEaPny4li9fbndJAAAgCIR0yHnzzTc1adIkPfTQQ/riiy90/vnnKzs7W6WlpXaXBgAAbBbSIeepp57Srbfeqptuukn9+/fXrFmz1K5dO/3973+3uzQAAGCzkA05NTU1KigoUFZWlrXN6XQqKytLeXl5Tb6nurpaXq/X7wUAAMJTyIacvXv3qr6+XikpKX7bU1JSVFxc3OR7pk+froSEBOuVkZHRGqUCAAAbtKmnkE+ZMkWTJk2y1j0ej7p168YZHQAAQkjj77Yx5qTtQjbkdOrUSRERESopKfHbXlJSotTU1Cbf43K55HK5rPXGTuKMDgAAoaeiokIJCQkn3B+yISc6OlpDhw7VokWLNHr0aEmSz+fTokWLNGHChGZ9Rnp6uoqKihQfHy+Hw9FitXm9XmVkZKioqEhut7vFPhf+6OfWQ1+3Dvq5ddDPrSOQ/WyMUUVFhdLT00/aLmRDjiRNmjRJY8eO1bBhw3TBBRfov//7v3XgwAHddNNNzXq/0+lU165dA1af2+3mf6BWQD+3Hvq6ddDPrYN+bh2B6ueTncFpFNIh52c/+5n27NmjadOmqbi4WIMGDdKCBQuOG4wMAADanpAOOZI0YcKEZl+eAgAAbUfITiEPZi6XSw899JDfIGe0PPq59dDXrYN+bh30c+sIhn52mO+afwUAABCCOJMDAADCEiEHAACEJUIOAAAIS4QcAAAQlgg5AfDcc8+pR48eiomJ0fDhw7V8+XK7Swpa06dP1/e+9z3Fx8crOTlZo0ePVmFhoV+bqqoqjR8/Xh07dlRcXJyuu+664x7nsXPnTuXk5Khdu3ZKTk7Wvffeq7q6Or82n3zyiYYMGSKXy6XevXtr9uzZgT68oDVjxgw5HA5NnDjR2kY/t4xdu3bp5z//uTp27KjY2FgNHDhQK1eutPYbYzRt2jSlpaUpNjZWWVlZ2rJli99nlJWVKTc3V263W4mJiRo3bpwqKyv92qxdu1YXX3yxYmJilJGRoZkzZ7bK8QWD+vp6TZ06VT179lRsbKzOOussPfLII37PMaKfT8/SpUt1zTXXKD09XQ6HQ++++67f/tbs17ffflt9+/ZVTEyMBg4cqPnz55/6ARm0qDfeeMNER0ebv//972bDhg3m1ltvNYmJiaakpMTu0oJSdna2efnll8369evN6tWrzVVXXWW6detmKisrrTa33367ycjIMIsWLTIrV640F154obnooous/XV1dWbAgAEmKyvLrFq1ysyfP9906tTJTJkyxWqzdetW065dOzNp0iSzceNG8+yzz5qIiAizYMGCVj3eYLB8+XLTo0cPc95555m77rrL2k4/n7mysjLTvXt388tf/tLk5+ebrVu3mg8//NB89dVXVpsZM2aYhIQE8+6775o1a9aYH/zgB6Znz57m0KFDVpsrrrjCnH/++WbZsmXm3//+t+ndu7e5/vrrrf0ej8ekpKSY3Nxcs379evP666+b2NhY85e//KVVj9cujz32mOnYsaOZO3eu2bZtm3n77bdNXFycefrpp6029PPpmT9/vnnggQfMP//5TyPJvPPOO377W6tfP/vsMxMREWFmzpxpNm7caB588EETFRVl1q1bd0rHQ8hpYRdccIEZP368tV5fX2/S09PN9OnTbawqdJSWlhpJZsmSJcYYY8rLy01UVJR5++23rTabNm0ykkxeXp4xpuF/SqfTaYqLi602L7zwgnG73aa6utoYY8x9991nzj33XL/v+tnPfmays7MDfUhBpaKiwpx99tlm4cKF5tJLL7VCDv3cMiZPnmxGjhx5wv0+n8+kpqaaJ554wtpWXl5uXC6Xef31140xxmzcuNFIMitWrLDafPDBB8bhcJhdu3YZY4x5/vnnTVJSktXvjd/dp0+flj6koJSTk2Nuvvlmv23XXnutyc3NNcbQzy3l2JDTmv3605/+1OTk5PjVM3z4cPOrX/3qlI6By1UtqKamRgUFBcrKyrK2OZ1OZWVlKS8vz8bKQofH45EkdejQQZJUUFCg2tpavz7t27evunXrZvVpXl6eBg4c6Pc4j+zsbHm9Xm3YsMFqc/RnNLZpa38u48ePV05OznF9QT+3jPfee0/Dhg3TT37yEyUnJ2vw4MF66aWXrP3btm1TcXGxXx8lJCRo+PDhfv2cmJioYcOGWW2ysrLkdDqVn59vtbnkkksUHR1ttcnOzlZhYaH2798f6MO03UUXXaRFixZp8+bNkqQ1a9bo008/1ZVXXimJfg6U1uzXlvq3hJDTgvbu3av6+vrjnp2VkpKi4uJim6oKHT6fTxMnTtSIESM0YMAASVJxcbGio6OVmJjo1/boPi0uLm6yzxv3nayN1+vVoUOHAnE4QeeNN97QF198oenTpx+3j35uGVu3btULL7ygs88+Wx9++KHuuOMO/eY3v9GcOXMkHemnk/0bUVxcrOTkZL/9kZGR6tChwyn9WYSz+++/X2PGjFHfvn0VFRWlwYMHa+LEicrNzZVEPwdKa/bridqcar+H/LOrED7Gjx+v9evX69NPP7W7lLBTVFSku+66SwsXLlRMTIzd5YQtn8+nYcOG6fHHH5ckDR48WOvXr9esWbM0duxYm6sLH2+99ZZeffVVvfbaazr33HO1evVqTZw4Uenp6fQz/HAmpwV16tRJERERx81IKSkpUWpqqk1VhYYJEyZo7ty5+vjjj9W1a1dre2pqqmpqalReXu7X/ug+TU1NbbLPG/edrI3b7VZsbGxLH07QKSgoUGlpqYYMGaLIyEhFRkZqyZIleuaZZxQZGamUlBT6uQWkpaWpf//+ftv69eunnTt3SjrSTyf7NyI1NVWlpaV+++vq6lRWVnZKfxbh7N5777XO5gwcOFA33nij7r77bussJf0cGK3Zrydqc6r9TshpQdHR0Ro6dKgWLVpkbfP5fFq0aJEyMzNtrCx4GWM0YcIEvfPOO1q8eLF69uzpt3/o0KGKiory69PCwkLt3LnT6tPMzEytW7fO73+shQsXyu12Wz84mZmZfp/R2Kat/LlcfvnlWrdunVavXm29hg0bptzcXGuZfj5zI0aMOO4WCJs3b1b37t0lST179lRqaqpfH3m9XuXn5/v1c3l5uQoKCqw2ixcvls/n0/Dhw602S5cuVW1trdVm4cKF6tOnj5KSkgJ2fMHi4MGDcjr9f74iIiLk8/kk0c+B0pr92mL/lpzSMGV8pzfeeMO4XC4ze/Zss3HjRnPbbbeZxMREvxkpOOKOO+4wCQkJ5pNPPjHffvut9Tp48KDV5vbbbzfdunUzixcvNitXrjSZmZkmMzPT2t84tXnUqFFm9erVZsGCBaZz585NTm2+9957zaZNm8xzzz3XpqY2N+Xo2VXG0M8tYfny5SYyMtI89thjZsuWLebVV1817dq1M6+88orVZsaMGSYxMdH861//MmvXrjU//OEPm5yCO3jwYJOfn28+/fRTc/bZZ/tNwS0vLzcpKSnmxhtvNOvXrzdvvPGGadeuXVhPbT7a2LFjTZcuXawp5P/85z9Np06dzH333We1oZ9PT0VFhVm1apVZtWqVkWSeeuops2rVKrNjxw5jTOv162effWYiIyPNH//4R7Np0ybz0EMPMYU8WDz77LOmW7duJjo62lxwwQVm2bJldpcUtCQ1+Xr55ZetNocOHTK//vWvTVJSkmnXrp350Y9+ZL799lu/z9m+fbu58sorTWxsrOnUqZO55557TG1trV+bjz/+2AwaNMhER0ebXr16+X1HW3RsyKGfW8b7779vBgwYYFwul+nbt6958cUX/fb7fD4zdepUk5KSYlwul7n88stNYWGhX5t9+/aZ66+/3sTFxRm3221uuukmU1FR4ddmzZo1ZuTIkcblcpkuXbqYGTNmBPzYgoXX6zV33XWX6datm4mJiTG9evUyDzzwgN+UZPr59Hz88cdN/ps8duxYY0zr9utbb71lzjnnHBMdHW3OPfdcM2/evFM+HocxR90iEgAAIEwwJgcAAIQlQg4AAAhLhBwAABCWCDkAACAsEXIAAEBYIuQAAICwRMgBAABhiZADAADCEiEHAACEJUIOAAAIS4QcAAAQlgg5AAAgLP1/X7EVFtSe8ucAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reference implementation using pytorch's .backward()\n",
    "Nothing to do in Step 2, this code is provided for you as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, b1, w2, b2 = init()\n",
    "parameters = [w1, b1, w2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference code\n",
    "torch.set_printoptions(linewidth=200)\n",
    "import torch.nn as F\n",
    "\n",
    "def train(w1, b1, w2, b2):\n",
    "    lossi = []\n",
    "    for step in range(10000):\n",
    "        xb = train_input\n",
    "        yb = train_target\n",
    "        num_samples = xb.shape[0]\n",
    "        # forward\n",
    "        z1, h1, z2, h2 = forward(w1, b1, w2, b2, xb)\n",
    "        xloss = F.MSELoss()\n",
    "        lsi = xloss(h2, yb) * yb.nelement()\n",
    "        # backward\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        lsi.backward()\n",
    "        # update\n",
    "        lr = 0.1 / num_samples\n",
    "        for p in parameters:\n",
    "            p.data += -lr * p.grad\n",
    "        if step % 100 == 0: print(f'step = {step}, loss = {lsi}')\n",
    "        lossi.append(lsi.item())\n",
    "    # compute accuracy\n",
    "    _, _, _, preds = forward(w1, b1, w2, b2, train_input)\n",
    "    train_accuracy = compute_accuracy(preds, train_target)\n",
    "    _, _, _, preds = forward(w1, b1, w2, b2, test_input)\n",
    "    test_accuracy = compute_accuracy(preds, test_target)\n",
    "    print(f'{train_accuracy=}')\n",
    "    print(f'{test_accuracy=}')\n",
    "    return lossi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0, loss = 9739.25390625\n",
      "step = 100, loss = 8368.6396484375\n",
      "step = 200, loss = 7992.68701171875\n",
      "step = 300, loss = 7720.29638671875\n",
      "step = 400, loss = 7503.93310546875\n",
      "step = 500, loss = 7352.43994140625\n",
      "step = 600, loss = 7179.36376953125\n",
      "step = 700, loss = 7036.6962890625\n",
      "step = 800, loss = 6904.39208984375\n",
      "step = 900, loss = 6768.669921875\n",
      "step = 1000, loss = 6660.0673828125\n",
      "step = 1100, loss = 6549.5\n",
      "step = 1200, loss = 6418.8115234375\n",
      "step = 1300, loss = 6258.0869140625\n",
      "step = 1400, loss = 6024.8515625\n",
      "step = 1500, loss = 5575.2978515625\n",
      "step = 1600, loss = 5364.59765625\n",
      "step = 1700, loss = 5095.408203125\n",
      "step = 1800, loss = 4635.765625\n",
      "step = 1900, loss = 4120.9599609375\n",
      "step = 2000, loss = 3448.3359375\n",
      "step = 2100, loss = 2568.770751953125\n",
      "step = 2200, loss = 2158.4833984375\n",
      "step = 2300, loss = 2116.86669921875\n",
      "step = 2400, loss = 2059.064453125\n",
      "step = 2500, loss = 1980.979248046875\n",
      "step = 2600, loss = 1891.409423828125\n",
      "step = 2700, loss = 1750.2545166015625\n",
      "step = 2800, loss = 1381.51318359375\n",
      "step = 2900, loss = 1302.1201171875\n",
      "step = 3000, loss = 1193.171630859375\n",
      "step = 3100, loss = 958.3279418945312\n",
      "step = 3200, loss = 913.3738403320312\n",
      "step = 3300, loss = 879.2062377929688\n",
      "step = 3400, loss = 860.0853271484375\n",
      "step = 3500, loss = 848.4304809570312\n",
      "step = 3600, loss = 839.7960205078125\n",
      "step = 3700, loss = 828.5515747070312\n",
      "step = 3800, loss = 815.9716186523438\n",
      "step = 3900, loss = 800.6820068359375\n",
      "step = 4000, loss = 788.4972534179688\n",
      "step = 4100, loss = 777.0404663085938\n",
      "step = 4200, loss = 763.3594970703125\n",
      "step = 4300, loss = 749.8787231445312\n",
      "step = 4400, loss = 735.1650390625\n",
      "step = 4500, loss = 713.2142333984375\n",
      "step = 4600, loss = 672.8694458007812\n",
      "step = 4700, loss = 581.3194580078125\n",
      "step = 4800, loss = 433.2962646484375\n",
      "step = 4900, loss = 417.7125244140625\n",
      "step = 5000, loss = 413.27471923828125\n",
      "step = 5100, loss = 410.3175964355469\n",
      "step = 5200, loss = 407.75396728515625\n",
      "step = 5300, loss = 405.3391418457031\n",
      "step = 5400, loss = 402.7843017578125\n",
      "step = 5500, loss = 400.2506408691406\n",
      "step = 5600, loss = 397.9172668457031\n",
      "step = 5700, loss = 395.7304382324219\n",
      "step = 5800, loss = 393.5703125\n",
      "step = 5900, loss = 391.23681640625\n",
      "step = 6000, loss = 388.66571044921875\n",
      "step = 6100, loss = 386.5531005859375\n",
      "step = 6200, loss = 384.72601318359375\n",
      "step = 6300, loss = 383.0723876953125\n",
      "step = 6400, loss = 381.58477783203125\n",
      "step = 6500, loss = 380.1479797363281\n",
      "step = 6600, loss = 378.4463806152344\n",
      "step = 6700, loss = 376.5823974609375\n",
      "step = 6800, loss = 374.7453308105469\n",
      "step = 6900, loss = 373.063720703125\n",
      "step = 7000, loss = 371.4705505371094\n",
      "step = 7100, loss = 370.0395812988281\n",
      "step = 7200, loss = 368.7829284667969\n",
      "step = 7300, loss = 367.6322021484375\n",
      "step = 7400, loss = 366.4996337890625\n",
      "step = 7500, loss = 365.3518981933594\n",
      "step = 7600, loss = 364.1800537109375\n",
      "step = 7700, loss = 363.03265380859375\n",
      "step = 7800, loss = 361.8948974609375\n",
      "step = 7900, loss = 360.8462219238281\n",
      "step = 8000, loss = 359.9231262207031\n",
      "step = 8100, loss = 359.0537109375\n",
      "step = 8200, loss = 358.18634033203125\n",
      "step = 8300, loss = 357.28106689453125\n",
      "step = 8400, loss = 356.3276062011719\n",
      "step = 8500, loss = 355.4635925292969\n",
      "step = 8600, loss = 354.63214111328125\n",
      "step = 8700, loss = 353.8135070800781\n",
      "step = 8800, loss = 352.9992980957031\n",
      "step = 8900, loss = 352.2085266113281\n",
      "step = 9000, loss = 351.4079895019531\n",
      "step = 9100, loss = 350.5335693359375\n",
      "step = 9200, loss = 349.52069091796875\n",
      "step = 9300, loss = 348.4715881347656\n",
      "step = 9400, loss = 347.4921875\n",
      "step = 9500, loss = 346.6770935058594\n",
      "step = 9600, loss = 345.8746032714844\n",
      "step = 9700, loss = 345.0944519042969\n",
      "step = 9800, loss = 344.3463439941406\n",
      "step = 9900, loss = 343.6240539550781\n",
      "train_accuracy=0.8709999918937683\n",
      "test_accuracy=0.597000002861023\n"
     ]
    }
   ],
   "source": [
    "lossi = train(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x225c4ffb8d0>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build the same MLP layer but with fully pytorch code (nn.Linear(), etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network dimensions\n",
    "n_in = 784\n",
    "n_hidden = 200\n",
    "n_out = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, Y_tr = train_input, train_target\n",
    "X_test, Y_test = test_input, test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_out = n_out\n",
    "        self.layers = nn.ModuleList((\n",
    "            nn.Linear(self.n_in, self.n_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.n_hidden, self.n_out),\n",
    "            nn.Tanh()\n",
    "        ))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def __parameters__(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters]\n",
    "\n",
    "model = MLP(n_in = 784, n_hidden = 50, n_out = 10)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 784]), torch.Size([1000, 10]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape, Y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =      0\tloss=0.14834\taccuracy (train, test): 0.10300\t0.19700\n",
      "step =   1000\tloss=0.00064\taccuracy (train, test): 1.00000\t0.84700\n",
      "step =   2000\tloss=0.00016\taccuracy (train, test): 1.00000\t0.83400\n",
      "step =   3000\tloss=0.00019\taccuracy (train, test): 1.00000\t0.82500\n",
      "step =   4000\tloss=0.00004\taccuracy (train, test): 1.00000\t0.81900\n",
      "step =   5000\tloss=0.00003\taccuracy (train, test): 1.00000\t0.81900\n",
      "step =   6000\tloss=0.00003\taccuracy (train, test): 1.00000\t0.81800\n",
      "step =   7000\tloss=0.00002\taccuracy (train, test): 1.00000\t0.82200\n",
      "step =   8000\tloss=0.00009\taccuracy (train, test): 1.00000\t0.82300\n",
      "step =   9000\tloss=0.00010\taccuracy (train, test): 1.00000\t0.82100\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "num_epochs = 10000\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    y_pred = model(X_tr)\n",
    "    loss = loss_fn(y_pred, Y_tr)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if n % 1000 == 0: \n",
    "        with torch.no_grad():\n",
    "            # train accuracy\n",
    "            acc_train = compute_accuracy(y_pred, Y_tr)\n",
    "            # test accuracy\n",
    "            y_test_preds = model(X_test)\n",
    "            acc_test = compute_accuracy(y_test_preds, Y_test)\n",
    "            print(f'step = {n:6d}\\tloss={loss.item():.5f}\\taccuracy (train, test): {acc_train:.5f}\\t{acc_test:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque qu' la fin de l'entranement, la prcision sur l'ensemble d'entranement est de 100% alors qu'elle est de 82,1% sur l'ensemble de test. Cela pourrait tre signe d'overfitting, le modle s'ajuste trop spcifiquement aux donnes d'entranement et n'est pas capable de gnraliser efficacement sur des donnes non vues. On va essayer d'amliorer la prcision sur l'ensemble de test quitte  diminuer celle sur l'ensemble d'entranement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: try to improve accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On va d'abord essayer d'augmenter le taux d'apprentissage 1e-3 -> 1e-1 ou 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(n_in = 784, n_hidden = 50, n_out = 10)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-1)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =      0\tloss=0.15288\taccuracy (train, test): 0.08000\t0.08800\n",
      "step =   1000\tloss=1.15618\taccuracy (train, test): 0.09100\t0.07900\n",
      "step =   2000\tloss=1.04284\taccuracy (train, test): 0.11600\t0.12600\n",
      "step =   3000\tloss=1.03672\taccuracy (train, test): 0.09700\t0.08500\n",
      "step =   4000\tloss=0.97876\taccuracy (train, test): 0.09700\t0.08500\n",
      "step =   5000\tloss=1.01008\taccuracy (train, test): 0.09700\t0.08500\n",
      "step =   6000\tloss=1.07272\taccuracy (train, test): 0.09700\t0.08500\n",
      "step =   7000\tloss=1.08568\taccuracy (train, test): 0.09700\t0.08500\n",
      "step =   8000\tloss=1.05004\taccuracy (train, test): 0.09700\t0.08500\n",
      "step =   9000\tloss=1.04032\taccuracy (train, test): 0.11600\t0.12600\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "num_epochs = 10000\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    y_pred = model(X_tr)\n",
    "    loss = loss_fn(y_pred, Y_tr)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if n % 1000 == 0: \n",
    "        with torch.no_grad():\n",
    "            # train accuracy\n",
    "            acc_train = compute_accuracy(y_pred, Y_tr)\n",
    "            # test accuracy\n",
    "            y_test_preds = model(X_test)\n",
    "            acc_test = compute_accuracy(y_test_preds, Y_test)\n",
    "            print(f'step = {n:6d}\\tloss={loss.item():.5f}\\taccuracy (train, test): {acc_train:.5f}\\t{acc_test:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prcision trs faible (pour ensemble d'entranement et de test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(n_in = 784, n_hidden = 50, n_out = 10)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =      0\tloss=0.14251\taccuracy (train, test): 0.12800\t0.14200\n",
      "step =   1000\tloss=0.00267\taccuracy (train, test): 0.98900\t0.75600\n",
      "step =   2000\tloss=0.00196\taccuracy (train, test): 0.99300\t0.74600\n",
      "step =   3000\tloss=0.00189\taccuracy (train, test): 0.99200\t0.74500\n",
      "step =   4000\tloss=0.00172\taccuracy (train, test): 0.99200\t0.73700\n",
      "step =   5000\tloss=0.00143\taccuracy (train, test): 0.99400\t0.73400\n",
      "step =   6000\tloss=0.00283\taccuracy (train, test): 0.98500\t0.76200\n",
      "step =   7000\tloss=0.00200\taccuracy (train, test): 0.99100\t0.76400\n",
      "step =   8000\tloss=0.00185\taccuracy (train, test): 0.99300\t0.77000\n",
      "step =   9000\tloss=0.00163\taccuracy (train, test): 0.99200\t0.77500\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "num_epochs = 10000\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    y_pred = model(X_tr)\n",
    "    loss = loss_fn(y_pred, Y_tr)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if n % 1000 == 0: \n",
    "        with torch.no_grad():\n",
    "            # train accuracy\n",
    "            acc_train = compute_accuracy(y_pred, Y_tr)\n",
    "            # test accuracy\n",
    "            y_test_preds = model(X_test)\n",
    "            acc_test = compute_accuracy(y_test_preds, Y_test)\n",
    "            print(f'step = {n:6d}\\tloss={loss.item():.5f}\\taccuracy (train, test): {acc_train:.5f}\\t{acc_test:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La prcision a diminu aussi bien sur l'ensamble d'entranement que sur l'ensemble de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Diminuer le nombre de neurones dans la couche cache : 50 -> 20 ou 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(n_in = 784, n_hidden = 20, n_out = 10)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =      0\tloss=0.18817\taccuracy (train, test): 0.05100\t0.10400\n",
      "step =   1000\tloss=0.00333\taccuracy (train, test): 0.99900\t0.82400\n",
      "step =   2000\tloss=0.00089\taccuracy (train, test): 1.00000\t0.80900\n",
      "step =   3000\tloss=0.00042\taccuracy (train, test): 1.00000\t0.79600\n",
      "step =   4000\tloss=0.00024\taccuracy (train, test): 1.00000\t0.78900\n",
      "step =   5000\tloss=0.00016\taccuracy (train, test): 1.00000\t0.79100\n",
      "step =   6000\tloss=0.00011\taccuracy (train, test): 1.00000\t0.78900\n",
      "step =   7000\tloss=0.00009\taccuracy (train, test): 1.00000\t0.78400\n",
      "step =   8000\tloss=0.00010\taccuracy (train, test): 1.00000\t0.78100\n",
      "step =   9000\tloss=0.00006\taccuracy (train, test): 1.00000\t0.78200\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "num_epochs = 10000\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    y_pred = model(X_tr)\n",
    "    loss = loss_fn(y_pred, Y_tr)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if n % 1000 == 0: \n",
    "        with torch.no_grad():\n",
    "            # train accuracy\n",
    "            acc_train = compute_accuracy(y_pred, Y_tr)\n",
    "            # test accuracy\n",
    "            y_test_preds = model(X_test)\n",
    "            acc_test = compute_accuracy(y_test_preds, Y_test)\n",
    "            print(f'step = {n:6d}\\tloss={loss.item():.5f}\\taccuracy (train, test): {acc_train:.5f}\\t{acc_test:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(n_in = 784, n_hidden = 30, n_out = 10)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =      0\tloss=0.14495\taccuracy (train, test): 0.06500\t0.12200\n",
      "step =   1000\tloss=0.00110\taccuracy (train, test): 1.00000\t0.82700\n",
      "step =   2000\tloss=0.00028\taccuracy (train, test): 1.00000\t0.80200\n",
      "step =   3000\tloss=0.00013\taccuracy (train, test): 1.00000\t0.79500\n",
      "step =   4000\tloss=0.00007\taccuracy (train, test): 1.00000\t0.78900\n",
      "step =   5000\tloss=0.00005\taccuracy (train, test): 1.00000\t0.79000\n",
      "step =   6000\tloss=0.00005\taccuracy (train, test): 1.00000\t0.79200\n",
      "step =   7000\tloss=0.00003\taccuracy (train, test): 1.00000\t0.79300\n",
      "step =   8000\tloss=0.00002\taccuracy (train, test): 1.00000\t0.78500\n",
      "step =   9000\tloss=0.00003\taccuracy (train, test): 1.00000\t0.78300\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "num_epochs = 10000\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    y_pred = model(X_tr)\n",
    "    loss = loss_fn(y_pred, Y_tr)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if n % 1000 == 0: \n",
    "        with torch.no_grad():\n",
    "            # train accuracy\n",
    "            acc_train = compute_accuracy(y_pred, Y_tr)\n",
    "            # test accuracy\n",
    "            y_test_preds = model(X_test)\n",
    "            acc_test = compute_accuracy(y_test_preds, Y_test)\n",
    "            print(f'step = {n:6d}\\tloss={loss.item():.5f}\\taccuracy (train, test): {acc_train:.5f}\\t{acc_test:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas trs concluant non plus..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modifier la fonction de perte : MSEloss -> CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(n_in = 784, n_hidden = 50, n_out = 10)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =      0\tloss=2.09453\taccuracy (train, test): 0.12500\t0.30200\n",
      "step =   1000\tloss=0.72467\taccuracy (train, test): 0.99300\t0.85900\n",
      "step =   2000\tloss=0.72415\taccuracy (train, test): 0.99400\t0.85900\n",
      "step =   3000\tloss=0.72409\taccuracy (train, test): 0.99400\t0.85500\n",
      "step =   4000\tloss=0.72406\taccuracy (train, test): 0.99400\t0.84800\n",
      "step =   5000\tloss=0.72405\taccuracy (train, test): 0.99400\t0.85200\n",
      "step =   6000\tloss=0.72404\taccuracy (train, test): 0.99400\t0.84700\n",
      "step =   7000\tloss=0.72403\taccuracy (train, test): 0.99400\t0.85300\n",
      "step =   8000\tloss=0.72403\taccuracy (train, test): 0.99400\t0.85100\n",
      "step =   9000\tloss=0.72403\taccuracy (train, test): 0.99400\t0.85000\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "num_epochs = 10000\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    y_pred = model(X_tr)\n",
    "    loss = loss_fn(y_pred, Y_tr)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if n % 1000 == 0: \n",
    "        with torch.no_grad():\n",
    "            # train accuracy\n",
    "            acc_train = compute_accuracy(y_pred, Y_tr)\n",
    "            # test accuracy\n",
    "            y_test_preds = model(X_test)\n",
    "            acc_test = compute_accuracy(y_test_preds, Y_test)\n",
    "            print(f'step = {n:6d}\\tloss={loss.item():.5f}\\taccuracy (train, test): {acc_train:.5f}\\t{acc_test:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien meilleur ! Il y a moins d'overfitting et la prcision sur l'ensemble de test a lgrement augment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(n_in = 784, n_hidden = 35, n_out = 10)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =      0\tloss=2.09125\taccuracy (train, test): 0.09400\t0.29900\n",
      "step =   1000\tloss=0.72433\taccuracy (train, test): 0.99500\t0.84800\n",
      "step =   2000\tloss=0.72393\taccuracy (train, test): 0.99500\t0.84000\n",
      "step =   3000\tloss=0.72383\taccuracy (train, test): 0.99500\t0.84500\n",
      "step =   4000\tloss=0.72379\taccuracy (train, test): 0.99500\t0.84700\n",
      "step =   5000\tloss=0.72376\taccuracy (train, test): 0.99500\t0.84700\n",
      "step =   6000\tloss=0.72375\taccuracy (train, test): 0.99500\t0.84500\n",
      "step =   7000\tloss=0.72374\taccuracy (train, test): 0.99500\t0.84400\n",
      "step =   8000\tloss=0.88926\taccuracy (train, test): 0.87600\t0.67100\n",
      "step =   9000\tloss=0.88162\taccuracy (train, test): 0.87900\t0.66900\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "num_epochs = 10000\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    y_pred = model(X_tr)\n",
    "    loss = loss_fn(y_pred, Y_tr)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if n % 1000 == 0: \n",
    "        with torch.no_grad():\n",
    "            # train accuracy\n",
    "            acc_train = compute_accuracy(y_pred, Y_tr)\n",
    "            # test accuracy\n",
    "            y_test_preds = model(X_test)\n",
    "            acc_test = compute_accuracy(y_test_preds, Y_test)\n",
    "            print(f'step = {n:6d}\\tloss={loss.item():.5f}\\taccuracy (train, test): {acc_train:.5f}\\t{acc_test:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moins bien..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utiliser la rgularisation L2 (weight_decay dans l'optimiseur Adamw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(n_in = 784, n_hidden = 50, n_out = 10)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =      0\tloss=2.08669\taccuracy (train, test): 0.15100\t0.30500\n",
      "step =   1000\tloss=0.72747\taccuracy (train, test): 0.99200\t0.85100\n",
      "step =   2000\tloss=0.72682\taccuracy (train, test): 0.99200\t0.85100\n",
      "step =   3000\tloss=0.72677\taccuracy (train, test): 0.99200\t0.84900\n",
      "step =   4000\tloss=0.72540\taccuracy (train, test): 0.99300\t0.85100\n",
      "step =   5000\tloss=0.72539\taccuracy (train, test): 0.99300\t0.85400\n",
      "step =   6000\tloss=0.72539\taccuracy (train, test): 0.99300\t0.85300\n",
      "step =   7000\tloss=0.72538\taccuracy (train, test): 0.99300\t0.85500\n",
      "step =   8000\tloss=0.72538\taccuracy (train, test): 0.99300\t0.85400\n",
      "step =   9000\tloss=0.72538\taccuracy (train, test): 0.99300\t0.85200\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "num_epochs = 10000\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    y_pred = model(X_tr)\n",
    "    loss = loss_fn(y_pred, Y_tr)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if n % 1000 == 0: \n",
    "        with torch.no_grad():\n",
    "            # train accuracy\n",
    "            acc_train = compute_accuracy(y_pred, Y_tr)\n",
    "            # test accuracy\n",
    "            y_test_preds = model(X_test)\n",
    "            acc_test = compute_accuracy(y_test_preds, Y_test)\n",
    "            print(f'step = {n:6d}\\tloss={loss.item():.5f}\\taccuracy (train, test): {acc_train:.5f}\\t{acc_test:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(n_in = 784, n_hidden = 50, n_out = 10)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =      0\tloss=2.09918\taccuracy (train, test): 0.08200\t0.34500\n",
      "step =   1000\tloss=0.72624\taccuracy (train, test): 0.99300\t0.85400\n",
      "step =   2000\tloss=0.72458\taccuracy (train, test): 0.99400\t0.84800\n",
      "step =   3000\tloss=0.72408\taccuracy (train, test): 0.99400\t0.85000\n",
      "step =   4000\tloss=0.72406\taccuracy (train, test): 0.99400\t0.85300\n",
      "step =   5000\tloss=0.72404\taccuracy (train, test): 0.99400\t0.85600\n",
      "step =   6000\tloss=0.72404\taccuracy (train, test): 0.99400\t0.84500\n",
      "step =   7000\tloss=0.72403\taccuracy (train, test): 0.99400\t0.84900\n",
      "step =   8000\tloss=0.72403\taccuracy (train, test): 0.99400\t0.84600\n",
      "step =   9000\tloss=0.72403\taccuracy (train, test): 0.99400\t0.84600\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "num_epochs = 10000\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    y_pred = model(X_tr)\n",
    "    loss = loss_fn(y_pred, Y_tr)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if n % 1000 == 0: \n",
    "        with torch.no_grad():\n",
    "            # train accuracy\n",
    "            acc_train = compute_accuracy(y_pred, Y_tr)\n",
    "            # test accuracy\n",
    "            y_test_preds = model(X_test)\n",
    "            acc_test = compute_accuracy(y_test_preds, Y_test)\n",
    "            print(f'step = {n:6d}\\tloss={loss.item():.5f}\\taccuracy (train, test): {acc_train:.5f}\\t{acc_test:.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(n_in = 784, n_hidden = 50, n_out = 10)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =      0\tloss=2.11816\taccuracy (train, test): 0.07300\t0.24800\n",
      "step =   1000\tloss=0.72573\taccuracy (train, test): 0.99300\t0.84500\n",
      "step =   2000\tloss=0.72414\taccuracy (train, test): 0.99400\t0.84600\n",
      "step =   3000\tloss=0.72408\taccuracy (train, test): 0.99400\t0.84200\n",
      "step =   4000\tloss=0.72406\taccuracy (train, test): 0.99400\t0.84200\n",
      "step =   5000\tloss=0.72405\taccuracy (train, test): 0.99400\t0.84400\n",
      "step =   6000\tloss=0.72404\taccuracy (train, test): 0.99400\t0.84300\n",
      "step =   7000\tloss=0.72403\taccuracy (train, test): 0.99400\t0.83700\n",
      "step =   8000\tloss=0.72403\taccuracy (train, test): 0.99400\t0.83500\n",
      "step =   9000\tloss=0.88279\taccuracy (train, test): 0.88600\t0.73500\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "num_epochs = 10000\n",
    "best_test_acc = 0  # Meilleure accuracy test\n",
    "patience = 500  # Nombre d'poques sans amlioration avant d'arrter\n",
    "counter = 0  # Compteur d'poques sans amlioration\n",
    "\n",
    "for n in range(num_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_tr)\n",
    "    \n",
    "    # Calcul de la loss\n",
    "    loss = loss_fn(y_pred, Y_tr)\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Vrifier l'accuracy tous les 1000 itrations\n",
    "    if n % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            # Accuracy sur le train set\n",
    "            acc_train = compute_accuracy(y_pred, Y_tr)\n",
    "            \n",
    "            # Accuracy sur le test set\n",
    "            y_test_preds = model(X_test)\n",
    "            acc_test = compute_accuracy(y_test_preds, Y_test)\n",
    "            \n",
    "            # Affichage des rsultats\n",
    "            print(f'step = {n:6d}\\tloss={loss.item():.5f}\\taccuracy (train, test): {acc_train:.5f}\\t{acc_test:.5f}')\n",
    "            \n",
    "            # Vrification de l'amlioration de l'accuracy sur le test set\n",
    "            if acc_test > best_test_acc:\n",
    "                best_test_acc = acc_test\n",
    "                counter = 0  # Rinitialiser le compteur si une amlioration est trouve\n",
    "            else:\n",
    "                counter += 1  # Incrmenter le compteur si l'accuracy ne s'amliore pas\n",
    "                \n",
    "                # Arrter l'entranement si l'accuracy ne s'amliore pas aprs un certain nombre d'poques\n",
    "                if counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ne fonctionne pas vraiment..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
